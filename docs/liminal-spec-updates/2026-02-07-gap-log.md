# Liminal Spec Gap Log (2026-02-07)

## Purpose

Track gaps found in this project's Feature Spec and Tech Design, then decide whether each gap should be addressed by improving the `liminal-spec` skill itself.

## Snapshot

This review compared:
- `docs/tech-design-mvp.md`
- `~/promptdb/docs/tech-arch/*.md`
- current local stack and package strategy

The key pattern: core protocol and flow design quality is strong, but cross-cutting architecture controls were under-specified until late (dependency baseline, NFR targets, threat model, env contract, and error contract).

## Gaps Found In Current Spec/Design

| ID | Gap | Impact | Added to Project Docs | Candidate Skill Improvement? |
|----|-----|--------|-----------------------|------------------------------|
| G-01 | No explicit dependency baseline strategy (runtime vs dev, version policy, optional packages) | Late churn in package choices and scripts | Yes | Yes |
| G-02 | Testing runner/tooling strategy decided late (Bun test vs Vitest projects) | Test organization and CI shape unclear | Partial | Yes |
| G-03 | Missing non-functional targets (latency, recovery, startup expectations) | Hard to evaluate readiness and regressions | Yes | Yes |
| G-04 | Missing constraints-accepted section | Hidden trade-offs stay implicit | Yes | Yes |
| G-05 | Missing threat model + input validation matrix | Security and robustness checks are ad hoc | Yes | Yes |
| G-06 | Missing runtime/env prerequisites matrix | Setup friction and hidden assumptions | Partial | Yes |
| G-07 | Missing machine-readable error contract (codes) | Inconsistent client error handling | Yes | Yes |
| G-08 | No explicit architecture pattern gate before execution | Important questions asked too late | Yes | Yes |
| G-09 | No explicit verification tiers (`verify` vs `verify-all`) in design-time planning | Inconsistent quality gates across stories and handoffs | Partial | Yes |
| G-10 | No policy for breaking up large tech design docs | Design readability and maintainability degrade as scope grows | Yes | Yes |
| G-11 | Parallel-agent guidance can over-emphasize coordination instead of hard task boundaries | Higher communication overhead, scope creep, and merge friction | Yes | Yes |
| G-12 | No explicit TDD phase commit boundary (Red completion checkpoint before Green implementation) | Harder audit trail and weaker protection against drifting directly to implementation | Yes | Yes |
| G-13 | No anti-reward-hacking check tied to modified test files during TDD Green | Risk of implementation gaming tests rather than satisfying intent | Yes | Yes |
| G-14 | Validator SOP does not explicitly include permissions needed for git-history checks | Validation quality depends on ad hoc permission assumptions | Yes | Yes |

## What PromptDB Did Well That We Should Reuse

1. Trade-off tables with explicit "Constraints Accepted".
2. Requirements section including measurable NFR targets.
3. Threat model and input-validation coverage tied to tests.
4. Environment variable and runtime contract as first-class design artifact.
5. Error response catalog with stable codes/messages.
6. Tooling consistency (Biome single tool for lint+format, Vitest projects for test segmentation).

## Proposed `liminal-spec` Skill Refinements

### Phase 2 (Feature Specification) additions

1. Add a required "Architecture Context Capture" mini-section:
   - Existing stack and runtime constraints
   - Required compatibility targets
   - Hard dependency preferences (if any)
2. Promote NFRs from optional to default-required for system features.
3. Add a mandatory "Assumptions to Validate Before Story 0" table.

### Phase 3 (Tech Design) additions

1. Add a required "Dependency and Tooling Baseline" section:
   - Runtime dependencies
   - Dev dependencies
   - Version policy (pinning strategy)
   - Script contract
   - Verification tiers (`verify`, `verify-all`) with exact command composition
2. Add required cross-cutting sections:
   - Constraints Accepted
   - Threat Model + Input Validation Matrix
   - Runtime Prerequisites + Environment Contract
   - Error Contract (machine-readable codes)
   - Non-functional target table
3. Add a "Phase 3 Gate: Architecture Pattern Checklist" before handoff to story sharding.
4. Add required "Design Document Decomposition Plan":
   - break-out thresholds
   - split strategy (by domain/altitude)
   - source-of-truth index doc

## Verification Protocol (Required in Tech Design)

Every Tech Design must define two verification tiers before Story 0 execution:

1. `verify` (regular verification): fast feedback gate used continuously during implementation.
2. `verify-all` (deep verification): broader gate for integration depth and release confidence.

### Required shape

- `verify` must include at least: `format:check`, `lint`, `typecheck`, and primary mocked/service test suites.
- `verify-all` must run `verify` plus integration suites, and include e2e suites when available.
- If integration/e2e suites do not exist yet, `verify-all` must still exist and run placeholders/no-op commands that return success with clear output.

### Example from this project

In `/Users/leemoore/code/liminal-builder/package.json`:

- `verify`: `bun run format:check && bun run lint && bun run typecheck && bun run test`
- `verify-all`: `bun run verify && bun run test:integration && bun run test:e2e`

This should be identified and created during Tech Design, not discovered ad hoc during execution.

## Tech Design Decomposition Policy (Required)

Tech designs naturally become large; decomposition must be intentional.

### Decision triggers for breakout docs

Break out into additional documents when any of these occurs:

1. Primary Tech Design exceeds ~1,500-2,000 lines or navigation overhead becomes material.
2. One domain (auth, protocol, UI architecture, testing, infra) grows dense enough to distract from core flow.
3. A section needs independent review cadence or ownership.

### Breakout method

1. Keep one index/root Tech Design document as the canonical map and decision log.
2. Split into focused companion docs (for example: `tech-design-testing.md`, `tech-design-protocol.md`, `tech-design-ui.md`).
3. Add explicit cross-links and "source of truth" ownership notes per section.
4. Preserve requirement traceability in each breakout doc (AC/TC mappings where relevant).

## Parallel Agent Planning Principle

When recommending parallel agent workflows, prefer hard-bounded work partitions over communication-heavy coordination patterns.

### Guidance

1. Partition work into independent paths with explicit file ownership.
2. Enforce strict no-cross-scope editing during parallel execution.
3. Avoid asking parallel agents to resolve cross-cutting coherence in-flight.
4. Run one explicit post-parallel coherence pass after all bounded tasks complete.

### Why

This reduces quadratic coordination costs, lowers merge/conflict risk, and keeps parallelism focused on truly independent work.

## TDD Integrity And Validator Permission Controls

### TDD phase boundary requirement

1. At the end of TDD Red for a story, create a commit checkpoint before entering TDD Green.
2. The checkpoint commit should include failing/expected-red tests and any skeleton scaffolding needed to express the behavior contract.
3. TDD Green proceeds only after this boundary exists, preserving a clear audit trail from requirement to red tests to implementation.

### Anti-reward-hacking checks in TDD Green

1. If test files are modified during Green, explicitly validate those test changes against the story AC/TC intent.
2. Validation must include review of changed assertions and coverage shape to ensure tests did not weaken intent.
3. Validators should inspect recent git history for affected test files to detect abrupt intent regression patterns.

### Validator permissions in SOP

1. Validator/subagent SOP guidance must declare required permissions for git/history inspection commands.
2. Typical minimum capability includes running `git log`, `git show`, and `git blame` on relevant test paths.
3. If permissions are restricted, SOP must define fallback behavior (flag incomplete validation instead of silently skipping history checks).

### Phase 4/5 (Story Sharding/Execution) additions

1. Require Story 0 prompt to include explicit setup verification:
   - dependency install success
   - scripts present and runnable
   - format/lint/typecheck/test baseline commands
2. Require validator prompts to check cross-cutting sections, not only module/interface completeness.

## New "Must Ask" Questions For Future Runs

Use these before freezing Phase 2 or Phase 3:

1. What existing stack patterns in adjacent projects should be reused?
2. Which runtime and package ecosystem versions are the compatibility target?
3. Do we need schema-first validation at all external boundaries? If yes, with what library?
4. How should tests be segmented (unit/ui/integration/e2e) and with what runner architecture?
5. What are the measurable NFR targets for startup, latency, reliability, and recovery?
6. What threats are in scope, and where are controls enforced and tested?
7. What env/runtime prerequisites are required locally and in CI?
8. What stable error code contract should clients rely on?
9. What is the `verify` command for regular development verification?
10. What is the `verify-all` command for deep verification, and what does it add beyond `verify`?
11. At what thresholds will Tech Design be split into breakout documents, and what is the split plan?
12. If parallel agents are used, what are the exact hard scope boundaries for each agent?
13. What is the single post-parallel coherence/reconciliation step?
14. Where is the TDD Red->Green commit boundary for each story, and what commit evidence is required?
15. If tests were changed in TDD Green, how will validators confirm those changes preserved AC/TC intent instead of weakening checks?
16. What validator agent permissions are required to run git-history checks on modified test files?

## Tracking Loop

For each newly discovered gap during this build:
1. Record it here with impact and mitigation.
2. Ask: "Can this be solved by improving `liminal-spec` guidance/checklists/templates?"
3. If yes, draft a specific skill update proposal (section + wording + phase).
4. Re-evaluate at story boundaries (after Story 0, after first integration story, before full execution wave).

## Next Candidate Improvements To Draft

1. Update `references/tech-design.md` with a cross-cutting architecture checklist template.
2. Update `references/feature-specification.md` to require architecture-context capture + default NFR table.
3. Add validation prompt snippets for dependency/tooling baseline and threat/input-validation review.
4. Add a standard verification-tier template (`verify` + `verify-all`) to Tech Design guidance and Story 0 prompt guidance.
5. Add a "when/how to split Tech Design docs" section to prevent oversized monolith design docs.
6. Add explicit parallelization guidance: hard boundaries first, one coherence pass after parallel execution.
7. Add TDD integrity guidance: mandatory Red->Green commit boundary plus validator review of changed tests in Green.
8. Add validator SOP guidance that explicitly includes permissions for git-history inspection of relevant test files.

## Recommended Skill File Changes (for Review)

| File | Recommended Change | Why |
|------|--------------------|-----|
| `/Users/leemoore/.claude/skills/liminal-spec/references/tech-design.md` | Add required "Cross-Cutting Architecture Controls" section with: dependency/tooling baseline, script contract, verification tiers (`verify`/`verify-all`), and doc decomposition policy. Extend handoff checklist to confirm these are defined. | G-01, G-02, G-09, and G-10 happened because architecture/process gates were implied but not explicitly required in Phase 3 guidance. |
| `/Users/leemoore/.claude/skills/liminal-spec/templates/tech-design.template.md` | Add template placeholders/tables for: resolved Tech Design Questions (including verification tiers + decomposition), script contract table, and decomposition thresholds. Add self-review checklist items for these sections. | Template-level nudges are the fastest way to make these controls consistently appear without relying on memory in each run. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/feature-specification.md` | In "Tech Design Questions" guidance/template, include required process questions: `verify`, `verify-all`, and split-plan thresholds. | If these questions are captured in Phase 2, Phase 3 must answer them before execution pressure starts. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/story-sharding.md` | Update Story 0 guidance so setup includes verification scripts and verify prompt expects regular/deep verification gates to be wired, with placeholders allowed when suites do not yet exist. | Story 0 is where project execution contract is created; missing script contract there causes drift and late rework. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/verification.md` | Add pre-story checkpoint items that confirm verification-tier commands are defined and a decomposition decision is documented. | This makes the phase gate enforce the process quality bar, not just module/interface completeness. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Add explicit orchestration rules for parallel execution: hard scope boundaries, no cross-scope edits, and one post-parallel coherence pass. | Parallel execution quality depends more on boundary discipline than in-flight coordination between many agents. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/phase-execution.md` | Add explicit Red->Green boundary control: commit at Red completion, then require Green validators to review any changed tests against AC/TC intent plus git-history signals. | Reinforces TDD integrity and reduces reward-hacking risk by making test-change scrutiny part of the process contract. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Extend validator SOP section to include required permissions/capabilities for test-file history inspection (`git log/show/blame`) and explicit fallback behavior when unavailable. | High-confidence validation requires historical context on test changes; this must be explicit, not assumed. |

## G-15: Story Validation Process (Agent Teams + Dual-Validator)

**Date:** 2026-02-07
**Context:** Ran full dual-validator story verification across 7 stories using Claude Code Agent Teams in tmux split panes + Codex GPT-5.3 subagents.

### What Happened
- 4 Opus teammates in tmux split panes, each launching 1-2 Codex (gpt-5.3-codex, high reasoning) subagents
- Phase 1: 7 parallel Codex verification runs (read-only, `-o` for output)
- Phase 2: Each Opus teammate validated every Codex claim against actual source files
- Phase 3: Adversarial debate — Codex sessions resumed with Opus pushback, Codex defended or conceded with evidence
- Phase 4: Fix list consolidated — all valid issues fixed, no severity-based deferrals

### Key Findings
1. Codex overstated severity on ~40% of issues; Opus caught design intent Codex missed
2. Codex was RIGHT and Opus was WRONG on 1 issue (TS compiler check on Record<CliType>)
3. Neither model alone was sufficient — the dual-validator pattern is essential
4. "Non-blocking" is not a valid reason to skip fixes. Fix everything unless genuinely unimportant AND difficult.
5. Wall-clock: ~30 min for what would be 4+ hours sequential

### Candidate Skill Improvement
- Draft reference doc: `.research/outputs/draft-validation-process-reference.md`
- Covers: agent teams setup, parallel Codex dispatch, dual-validator pattern, adversarial debate protocol, fix-everything standard
- To be reviewed for possible inclusion in liminal-spec skill as `references/story-validation-teams.md`

## G-16: Per-Story Implementation Workflow (Self-Review + Dual Verification)

**Date:** 2026-02-07
**Context:** Executed Story 1 (Project Sidebar) using a structured per-story workflow with Codex implementation, same-session self-review, and parallel dual verification.

### What Was Documented

The existing skill references (`phase-execution.md`, `execution-orchestration.md`) define the story cycle phases and agent coordination patterns, but do not specify:

1. **Same-session self-review** — After each implementation phase (Red, Green), the implementing agent critically reviews its own work before handoff. Cheaper than verification, catches 60-70% of issues.
2. **Dual verification details** — Codex + Senior Engineer (Opus) run the same verify prompt in parallel. Consolidated report presented to human.
3. **Human-gated fix cycles** — Human decides fix agent, scope, and when to move on. 1-3 rounds typical.
4. **Session management specifics** — When to resume vs fresh session, `resume` flag limitations, session ID tracking.
5. **Commit boundary protocol** — Required commits at Red and Green checkpoints with specific message conventions.
6. **Test modification policy for Green** — Green should not modify tests; environmental fixes flagged, assertion changes rejected.

### Gap

The skill's Phase 5 references describe *what* happens (Skeleton → Red → Green → Verify) and *how agents coordinate* (dual-validator, pipeline), but not the *specific operational workflow* including self-review passes, session resume patterns, commit discipline, and the human-gated fix cycle. This is the missing "how to actually run a story" reference.

### Draft Document

`docs/liminal-spec-updates/draft-story-execution-workflow.md`

### Candidate Skill Improvement?

**Yes.** This should become a companion reference to `phase-execution.md` and `execution-orchestration.md`, or be merged into one of them. The workflow is model-agnostic (works with any Codex-class + Opus-class pairing) and applies to any project using the liminal-spec methodology.

### Observations from Story 1

1. Self-review caught and fixed 5 issues + 2 nits during Red, and 3 issues during Green — all before verification
2. Dual verification found 1 legitimate production bug (Codex C) and 3 nits (Senior Engineer)
3. Codex resume syntax has gotchas: no `-C`, `--sandbox`, `-m`, `-c`, or `-o` flags on resume
4. Green modified test files (environment shims) — detected but current prompts are too permissive about this (see G-13)

---

## G-17: Orchestrator Transparency — Small Issues Buried During Execution

**Date:** 2026-02-07
**Context:** Story 2a (ACP Client) execution through full workflow. Post-execution review revealed the orchestrator (Opus) had noticed but not raised multiple issues during the run.

### What Happened

After completing Story 2a through dual verification, a direct question — "give me a list of all the issues you aren't telling me about" — surfaced 10 items the orchestrator had observed but not reported. These ranged from process gaps (Red never ran `bun run verify`) to spec violations (`close()` ignoring its `timeoutMs` parameter) to code quality concerns (`any` types in the prompt template, dead code from Story 1). Some were real problems requiring fixes; others were already clean but the orchestrator hadn't verified.

Additionally, 13 unresolved issues from Story 1 execution were discovered in a separate review — including a data path deviation from spec, dead test shims, exposed internal state (`public store`), and missing test cleanup — none of which the orchestrator had inventoried or raised proactively.

### Root Cause

The orchestrator defaults to forward momentum. When implementation agents or verifiers categorize something as a "nit" or "acceptable for MVP," the orchestrator accepts that framing without pushing back or accumulating the item for human review. The workflow doc defines the human as the gate, but the orchestrator filters what reaches the human.

### What Should Change

1. **Post-phase issue inventory.** After each major phase (Red complete, Green complete, verification complete), the orchestrator must produce a complete inventory of everything observed — not just blockers. Include items verifiers downgraded, process deviations, spec divergences, and code quality notes. Let the human decide what's small.

2. **No agent-determined severity downgrades.** When a verifier calls something a "nit," the orchestrator should report it at face value to the human rather than silently accepting the downgrade. The human may disagree about severity.

3. **Cross-story accumulation.** Issues that affect shared infrastructure (Story 0 code, store behavior, data paths, test patterns) must be flagged as cross-story concerns, not buried in per-story notes. These compound if deferred.

4. **Explicit "what I noticed but didn't flag" checkpoint.** Before the orchestrator declares a story complete, it should self-audit: "What did I observe during this execution that I haven't surfaced?" This is cheap insurance against the forward-momentum bias.

### Likely Fix: User-Initiated Prompt

Instructing the orchestrator to self-audit is unreliable — the same forward-momentum bias that causes the filtering will undermine instructions to stop filtering. This is probably best addressed with a **standard configured prompt that the user runs at the end of each story** (or at phase boundaries), something like:

> "List every issue you observed during this execution that you haven't raised — process gaps, spec divergences, severity downgrades by verifiers, code quality concerns, cross-story impacts, things you considered small. Do not filter. I will triage."

This should be documented as a required user action in the workflow, not as an orchestrator self-check. The user is the forcing function.

### Candidate Skill Improvement?

**Yes.** The execution orchestration reference (`execution-orchestration.md`) and the story execution workflow doc should include:
- A documented "transparency prompt" the user runs at story completion (not optional)
- A prohibition on the orchestrator filtering severity — report everything, let the human triage
- A cross-story concern accumulator that persists across story boundaries

### Items Found During This Review

| Item | Category | Resolution |
|------|----------|------------|
| Red phase never ran `bun run verify` | Process gap | Fixed — added `bun run verify` to all Red prompts (9 files) |
| `close(timeoutMs)` ignored timeout | Spec violation | Fixed — implemented `Promise.race` timeout |
| `handleAgentRequest` missing params | Spec divergence | Fixed — params now passed through |
| Data path `./data/` vs `~/.liminal-builder/` | Spec deviation | Fixed — corrected to homedir path |
| Dead Bun shim in project-store tests | Dead code | Fixed — removed |
| `public store` on ProjectStore | Encapsulation leak | Fixed — changed to private, tests use public API |
| JsonStore `writeDebounceMs <= 0` undocumented | Cross-story concern | Fixed — documented in JSDoc |
| Missing `afterEach` in client tests | Test hygiene | Fixed — added |
| Vitest/Node vs Bun runtime mismatch | Systemic | Documented in tech design |
| `any` types in green prompt template | Prompt quality | Not a current code issue (Codex B used proper types), but prompt should be cleaned for reuse |

---

## Suggested Acceptance Criteria For These Skill Updates

1. A new feature using liminal-spec cannot finish Phase 3 without explicit `verify` and `verify-all` commands.
2. Story 0 prompts require runnable verification commands, even if integration/e2e are placeholders.
3. Tech Design includes an explicit "split/no-split" decision with thresholds and rationale.
4. Verification checkpoints fail if these process artifacts are absent.
5. Parallel agent plans define hard file/task boundaries and a single explicit post-parallel coherence step.
6. Story execution guidance requires a Red-phase commit boundary before Green implementation begins.
7. When tests are modified in Green, validation requires both intent review against AC/TCs and git-history inspection of impacted test files.
8. Validator SOP documents required permissions for git/history checks, and validation reports must state when those checks could not be performed.
