# Liminal Spec Gap Log (2026-02-07)

## Purpose

Track gaps found in this project's Feature Spec and Tech Design, then decide whether each gap should be addressed by improving the `liminal-spec` skill itself.

## Snapshot

This review compared:
- `docs/tech-design-mvp.md`
- `~/promptdb/docs/tech-arch/*.md`
- current local stack and package strategy

The key pattern: core protocol and flow design quality is strong, but cross-cutting architecture controls were under-specified until late (dependency baseline, NFR targets, threat model, env contract, and error contract).

## Gaps Found In Current Spec/Design

| ID | Gap | Impact | Added to Project Docs | Candidate Skill Improvement? |
|----|-----|--------|-----------------------|------------------------------|
| G-01 | No explicit dependency baseline strategy (runtime vs dev, version policy, optional packages) | Late churn in package choices and scripts | Yes | Yes |
| G-02 | Testing runner/tooling strategy decided late (Bun test vs Vitest projects) | Test organization and CI shape unclear | Partial | Yes |
| G-03 | Missing non-functional targets (latency, recovery, startup expectations) | Hard to evaluate readiness and regressions | Yes | Yes |
| G-04 | Missing constraints-accepted section | Hidden trade-offs stay implicit | Yes | Yes |
| G-05 | Missing threat model + input validation matrix | Security and robustness checks are ad hoc | Yes | Yes |
| G-06 | Missing runtime/env prerequisites matrix | Setup friction and hidden assumptions | Partial | Yes |
| G-07 | Missing machine-readable error contract (codes) | Inconsistent client error handling | Yes | Yes |
| G-08 | No explicit architecture pattern gate before execution | Important questions asked too late | Yes | Yes |
| G-09 | No explicit verification tiers (`verify` vs `verify-all`) in design-time planning | Inconsistent quality gates across stories and handoffs | Partial | Yes |
| G-10 | No policy for breaking up large tech design docs | Design readability and maintainability degrade as scope grows | Yes | Yes |
| G-11 | Parallel-agent guidance can over-emphasize coordination instead of hard task boundaries | Higher communication overhead, scope creep, and merge friction | Yes | Yes |
| G-12 | No explicit TDD phase commit boundary (Red completion checkpoint before Green implementation) | Harder audit trail and weaker protection against drifting directly to implementation | Yes | Yes |
| G-13 | No anti-reward-hacking check tied to modified test files during TDD Green | Risk of implementation gaming tests rather than satisfying intent | Yes | Yes |
| G-14 | Validator SOP does not explicitly include permissions needed for git-history checks | Validation quality depends on ad hoc permission assumptions | Yes | Yes |

## What PromptDB Did Well That We Should Reuse

1. Trade-off tables with explicit "Constraints Accepted".
2. Requirements section including measurable NFR targets.
3. Threat model and input-validation coverage tied to tests.
4. Environment variable and runtime contract as first-class design artifact.
5. Error response catalog with stable codes/messages.
6. Tooling consistency (Biome single tool for lint+format, Vitest projects for test segmentation).

## Proposed `liminal-spec` Skill Refinements

### Phase 2 (Feature Specification) additions

1. Add a required "Architecture Context Capture" mini-section:
   - Existing stack and runtime constraints
   - Required compatibility targets
   - Hard dependency preferences (if any)
2. Promote NFRs from optional to default-required for system features.
3. Add a mandatory "Assumptions to Validate Before Story 0" table.

### Phase 3 (Tech Design) additions

1. Add a required "Dependency and Tooling Baseline" section:
   - Runtime dependencies
   - Dev dependencies
   - Version policy (pinning strategy)
   - Script contract
   - Verification tiers (`verify`, `verify-all`) with exact command composition
2. Add required cross-cutting sections:
   - Constraints Accepted
   - Threat Model + Input Validation Matrix
   - Runtime Prerequisites + Environment Contract
   - Error Contract (machine-readable codes)
   - Non-functional target table
3. Add a "Phase 3 Gate: Architecture Pattern Checklist" before handoff to story sharding.
4. Add required "Design Document Decomposition Plan":
   - break-out thresholds
   - split strategy (by domain/altitude)
   - source-of-truth index doc

## Verification Protocol (Required in Tech Design)

Every Tech Design must define two verification tiers before Story 0 execution:

1. `verify` (regular verification): fast feedback gate used continuously during implementation.
2. `verify-all` (deep verification): broader gate for integration depth and release confidence.

### Required shape

- `verify` must include at least: `format:check`, `lint`, `typecheck`, and primary mocked/service test suites.
- `verify-all` must run `verify` plus integration suites, and include e2e suites when available.
- If integration/e2e suites do not exist yet, `verify-all` must still exist and run placeholders/no-op commands that return success with clear output.

### Example from this project

In `/Users/leemoore/code/liminal-builder/package.json`:

- `verify`: `bun run format:check && bun run lint && bun run typecheck && bun run test`
- `verify-all`: `bun run verify && bun run test:integration && bun run test:e2e`

This should be identified and created during Tech Design, not discovered ad hoc during execution.

## Tech Design Decomposition Policy (Required)

Tech designs naturally become large; decomposition must be intentional.

### Decision triggers for breakout docs

Break out into additional documents when any of these occurs:

1. Primary Tech Design exceeds ~1,500-2,000 lines or navigation overhead becomes material.
2. One domain (auth, protocol, UI architecture, testing, infra) grows dense enough to distract from core flow.
3. A section needs independent review cadence or ownership.

### Breakout method

1. Keep one index/root Tech Design document as the canonical map and decision log.
2. Split into focused companion docs (for example: `tech-design-testing.md`, `tech-design-protocol.md`, `tech-design-ui.md`).
3. Add explicit cross-links and "source of truth" ownership notes per section.
4. Preserve requirement traceability in each breakout doc (AC/TC mappings where relevant).

## Parallel Agent Planning Principle

When recommending parallel agent workflows, prefer hard-bounded work partitions over communication-heavy coordination patterns.

### Guidance

1. Partition work into independent paths with explicit file ownership.
2. Enforce strict no-cross-scope editing during parallel execution.
3. Avoid asking parallel agents to resolve cross-cutting coherence in-flight.
4. Run one explicit post-parallel coherence pass after all bounded tasks complete.

### Why

This reduces quadratic coordination costs, lowers merge/conflict risk, and keeps parallelism focused on truly independent work.

## TDD Integrity And Validator Permission Controls

### TDD phase boundary requirement

1. At the end of TDD Red for a story, create a commit checkpoint before entering TDD Green.
2. The checkpoint commit should include failing/expected-red tests and any skeleton scaffolding needed to express the behavior contract.
3. TDD Green proceeds only after this boundary exists, preserving a clear audit trail from requirement to red tests to implementation.

### Anti-reward-hacking checks in TDD Green

1. If test files are modified during Green, explicitly validate those test changes against the story AC/TC intent.
2. Validation must include review of changed assertions and coverage shape to ensure tests did not weaken intent.
3. Validators should inspect recent git history for affected test files to detect abrupt intent regression patterns.

### Validator permissions in SOP

1. Validator/subagent SOP guidance must declare required permissions for git/history inspection commands.
2. Typical minimum capability includes running `git log`, `git show`, and `git blame` on relevant test paths.
3. If permissions are restricted, SOP must define fallback behavior (flag incomplete validation instead of silently skipping history checks).

### Phase 4/5 (Story Sharding/Execution) additions

1. Require Story 0 prompt to include explicit setup verification:
   - dependency install success
   - scripts present and runnable
   - format/lint/typecheck/test baseline commands
2. Require validator prompts to check cross-cutting sections, not only module/interface completeness.

## New "Must Ask" Questions For Future Runs

Use these before freezing Phase 2 or Phase 3:

1. What existing stack patterns in adjacent projects should be reused?
2. Which runtime and package ecosystem versions are the compatibility target?
3. Do we need schema-first validation at all external boundaries? If yes, with what library?
4. How should tests be segmented (unit/ui/integration/e2e) and with what runner architecture?
5. What are the measurable NFR targets for startup, latency, reliability, and recovery?
6. What threats are in scope, and where are controls enforced and tested?
7. What env/runtime prerequisites are required locally and in CI?
8. What stable error code contract should clients rely on?
9. What is the `verify` command for regular development verification?
10. What is the `verify-all` command for deep verification, and what does it add beyond `verify`?
11. At what thresholds will Tech Design be split into breakout documents, and what is the split plan?
12. If parallel agents are used, what are the exact hard scope boundaries for each agent?
13. What is the single post-parallel coherence/reconciliation step?
14. Where is the TDD Red->Green commit boundary for each story, and what commit evidence is required?
15. If tests were changed in TDD Green, how will validators confirm those changes preserved AC/TC intent instead of weakening checks?
16. What validator agent permissions are required to run git-history checks on modified test files?

## Tracking Loop

For each newly discovered gap during this build:
1. Record it here with impact and mitigation.
2. Ask: "Can this be solved by improving `liminal-spec` guidance/checklists/templates?"
3. If yes, draft a specific skill update proposal (section + wording + phase).
4. Re-evaluate at story boundaries (after Story 0, after first integration story, before full execution wave).

## Next Candidate Improvements To Draft

1. Update `references/tech-design.md` with a cross-cutting architecture checklist template.
2. Update `references/feature-specification.md` to require architecture-context capture + default NFR table.
3. Add validation prompt snippets for dependency/tooling baseline and threat/input-validation review.
4. Add a standard verification-tier template (`verify` + `verify-all`) to Tech Design guidance and Story 0 prompt guidance.
5. Add a "when/how to split Tech Design docs" section to prevent oversized monolith design docs.
6. Add explicit parallelization guidance: hard boundaries first, one coherence pass after parallel execution.
7. Add TDD integrity guidance: mandatory Red->Green commit boundary plus validator review of changed tests in Green.
8. Add validator SOP guidance that explicitly includes permissions for git-history inspection of relevant test files.

## Recommended Skill File Changes (for Review)

| File | Recommended Change | Why |
|------|--------------------|-----|
| `/Users/leemoore/.claude/skills/liminal-spec/references/tech-design.md` | Add required "Cross-Cutting Architecture Controls" section with: dependency/tooling baseline, script contract, verification tiers (`verify`/`verify-all`), and doc decomposition policy. Extend handoff checklist to confirm these are defined. | G-01, G-02, G-09, and G-10 happened because architecture/process gates were implied but not explicitly required in Phase 3 guidance. |
| `/Users/leemoore/.claude/skills/liminal-spec/templates/tech-design.template.md` | Add template placeholders/tables for: resolved Tech Design Questions (including verification tiers + decomposition), script contract table, and decomposition thresholds. Add self-review checklist items for these sections. | Template-level nudges are the fastest way to make these controls consistently appear without relying on memory in each run. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/feature-specification.md` | In "Tech Design Questions" guidance/template, include required process questions: `verify`, `verify-all`, and split-plan thresholds. | If these questions are captured in Phase 2, Phase 3 must answer them before execution pressure starts. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/story-sharding.md` | Update Story 0 guidance so setup includes verification scripts and verify prompt expects regular/deep verification gates to be wired, with placeholders allowed when suites do not yet exist. | Story 0 is where project execution contract is created; missing script contract there causes drift and late rework. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/verification.md` | Add pre-story checkpoint items that confirm verification-tier commands are defined and a decomposition decision is documented. | This makes the phase gate enforce the process quality bar, not just module/interface completeness. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Add explicit orchestration rules for parallel execution: hard scope boundaries, no cross-scope edits, and one post-parallel coherence pass. | Parallel execution quality depends more on boundary discipline than in-flight coordination between many agents. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/phase-execution.md` | Add explicit Red->Green boundary control: commit at Red completion, then require Green validators to review any changed tests against AC/TC intent plus git-history signals. | Reinforces TDD integrity and reduces reward-hacking risk by making test-change scrutiny part of the process contract. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Extend validator SOP section to include required permissions/capabilities for test-file history inspection (`git log/show/blame`) and explicit fallback behavior when unavailable. | High-confidence validation requires historical context on test changes; this must be explicit, not assumed. |

## G-15: Story Validation Process (Agent Teams + Dual-Validator)

**Date:** 2026-02-07
**Context:** Ran full dual-validator story verification across 7 stories using Claude Code Agent Teams in tmux split panes + Codex GPT-5.3 subagents.

### What Happened
- 4 Opus teammates in tmux split panes, each launching 1-2 Codex (gpt-5.3-codex, high reasoning) subagents
- Phase 1: 7 parallel Codex verification runs (read-only, `-o` for output)
- Phase 2: Each Opus teammate validated every Codex claim against actual source files
- Phase 3: Adversarial debate — Codex sessions resumed with Opus pushback, Codex defended or conceded with evidence
- Phase 4: Fix list consolidated — all valid issues fixed, no severity-based deferrals

### Key Findings
1. Codex overstated severity on ~40% of issues; Opus caught design intent Codex missed
2. Codex was RIGHT and Opus was WRONG on 1 issue (TS compiler check on Record<CliType>)
3. Neither model alone was sufficient — the dual-validator pattern is essential
4. "Non-blocking" is not a valid reason to skip fixes. Fix everything unless genuinely unimportant AND difficult.
5. Wall-clock: ~30 min for what would be 4+ hours sequential

### Candidate Skill Improvement
- Draft reference doc: `.research/outputs/draft-validation-process-reference.md`
- Covers: agent teams setup, parallel Codex dispatch, dual-validator pattern, adversarial debate protocol, fix-everything standard
- To be reviewed for possible inclusion in liminal-spec skill as `references/story-validation-teams.md`

## G-16: Per-Story Implementation Workflow (Self-Review + Dual Verification)

**Date:** 2026-02-07
**Context:** Executed Story 1 (Project Sidebar) using a structured per-story workflow with Codex implementation, same-session self-review, and parallel dual verification.

### What Was Documented

The existing skill references (`phase-execution.md`, `execution-orchestration.md`) define the story cycle phases and agent coordination patterns, but do not specify:

1. **Same-session self-review** — After each implementation phase (Red, Green), the implementing agent critically reviews its own work before handoff. Cheaper than verification, catches 60-70% of issues.
2. **Dual verification details** — Codex + Senior Engineer (Opus) run the same verify prompt in parallel. Consolidated report presented to human.
3. **Human-gated fix cycles** — Human decides fix agent, scope, and when to move on. 1-3 rounds typical.
4. **Session management specifics** — When to resume vs fresh session, `resume` flag limitations, session ID tracking.
5. **Commit boundary protocol** — Required commits at Red and Green checkpoints with specific message conventions.
6. **Test modification policy for Green** — Green should not modify tests; environmental fixes flagged, assertion changes rejected.

### Gap

The skill's Phase 5 references describe *what* happens (Skeleton → Red → Green → Verify) and *how agents coordinate* (dual-validator, pipeline), but not the *specific operational workflow* including self-review passes, session resume patterns, commit discipline, and the human-gated fix cycle. This is the missing "how to actually run a story" reference.

### Draft Document

`docs/liminal-spec-updates/draft-story-execution-workflow.md`

### Candidate Skill Improvement?

**Yes.** This should become a companion reference to `phase-execution.md` and `execution-orchestration.md`, or be merged into one of them. The workflow is model-agnostic (works with any Codex-class + Opus-class pairing) and applies to any project using the liminal-spec methodology.

### Observations from Story 1

1. Self-review caught and fixed 5 issues + 2 nits during Red, and 3 issues during Green — all before verification
2. Dual verification found 1 legitimate production bug (Codex C) and 3 nits (Senior Engineer)
3. Codex resume syntax has gotchas: no `-C`, `--sandbox`, `-m`, `-c`, or `-o` flags on resume
4. Green modified test files (environment shims) — detected but current prompts are too permissive about this (see G-13)

---

## G-17: Orchestrator Transparency — Small Issues Buried During Execution

**Date:** 2026-02-07
**Context:** Story 2a (ACP Client) execution through full workflow. Post-execution review revealed the orchestrator (Opus) had noticed but not raised multiple issues during the run.

### What Happened

After completing Story 2a through dual verification, a direct question — "give me a list of all the issues you aren't telling me about" — surfaced 10 items the orchestrator had observed but not reported. These ranged from process gaps (Red never ran `bun run verify`) to spec violations (`close()` ignoring its `timeoutMs` parameter) to code quality concerns (`any` types in the prompt template, dead code from Story 1). Some were real problems requiring fixes; others were already clean but the orchestrator hadn't verified.

Additionally, 13 unresolved issues from Story 1 execution were discovered in a separate review — including a data path deviation from spec, dead test shims, exposed internal state (`public store`), and missing test cleanup — none of which the orchestrator had inventoried or raised proactively.

### Root Cause

The orchestrator defaults to forward momentum. When implementation agents or verifiers categorize something as a "nit" or "acceptable for MVP," the orchestrator accepts that framing without pushing back or accumulating the item for human review. The workflow doc defines the human as the gate, but the orchestrator filters what reaches the human.

### What Should Change

1. **Post-phase issue inventory.** After each major phase (Red complete, Green complete, verification complete), the orchestrator must produce a complete inventory of everything observed — not just blockers. Include items verifiers downgraded, process deviations, spec divergences, and code quality notes. Let the human decide what's small.

2. **No agent-determined severity downgrades.** When a verifier calls something a "nit," the orchestrator should report it at face value to the human rather than silently accepting the downgrade. The human may disagree about severity.

3. **Cross-story accumulation.** Issues that affect shared infrastructure (Story 0 code, store behavior, data paths, test patterns) must be flagged as cross-story concerns, not buried in per-story notes. These compound if deferred.

4. **Explicit "what I noticed but didn't flag" checkpoint.** Before the orchestrator declares a story complete, it should self-audit: "What did I observe during this execution that I haven't surfaced?" This is cheap insurance against the forward-momentum bias.

### Likely Fix: User-Initiated Prompt

Instructing the orchestrator to self-audit is unreliable — the same forward-momentum bias that causes the filtering will undermine instructions to stop filtering. This is probably best addressed with a **standard configured prompt that the user runs at the end of each story** (or at phase boundaries), something like:

> "List every issue you observed during this execution that you haven't raised — process gaps, spec divergences, severity downgrades by verifiers, code quality concerns, cross-story impacts, things you considered small. Do not filter. I will triage."

This should be documented as a required user action in the workflow, not as an orchestrator self-check. The user is the forcing function.

### Candidate Skill Improvement?

**Yes.** The execution orchestration reference (`execution-orchestration.md`) and the story execution workflow doc should include:
- A documented "transparency prompt" the user runs at story completion (not optional)
- A prohibition on the orchestrator filtering severity — report everything, let the human triage
- A cross-story concern accumulator that persists across story boundaries

### Items Found During This Review

| Item | Category | Resolution |
|------|----------|------------|
| Red phase never ran `bun run verify` | Process gap | Fixed — added `bun run verify` to all Red prompts (9 files) |
| `close(timeoutMs)` ignored timeout | Spec violation | Fixed — implemented `Promise.race` timeout |
| `handleAgentRequest` missing params | Spec divergence | Fixed — params now passed through |
| Data path `./data/` vs `~/.liminal-builder/` | Spec deviation | Fixed — corrected to homedir path |
| Dead Bun shim in project-store tests | Dead code | Fixed — removed |
| `public store` on ProjectStore | Encapsulation leak | Fixed — changed to private, tests use public API |
| JsonStore `writeDebounceMs <= 0` undocumented | Cross-story concern | Fixed — documented in JSDoc |
| Missing `afterEach` in client tests | Test hygiene | Fixed — added |
| Vitest/Node vs Bun runtime mismatch | Systemic | Documented in tech design |
| `any` types in green prompt template | Prompt quality | Not a current code issue (Codex B used proper types), but prompt should be cleaned for reuse |

---

## G-18: No lint verification gate at TDD Red phase

**Date:** 2026-02-07
**Context:** Story 2b (Agent Manager) execution. Skeleton+Red prompt contained `any` types throughout DI interfaces and test mocks. Red completed with passing typecheck and correct test failure pattern, but Biome lint (with `--error-on-warnings`) rejected the code. This was not discovered until after Green completed, causing churn and a lost Green implementation.

### What Happened

The Red exit criteria only checked typecheck and tests. The project's `bun run verify` includes Biome lint with `--error-on-warnings`, which treats `noExplicitAny` as a hard failure. The Red prompt's code templates used `any` freely at DI boundaries and in test mocks. The Red self-review ran typecheck and tests but not lint. The problem cascaded forward: Green implemented against lint-dirty code, verification failed on lint, then the orchestrator tried to fix lint issues directly and destroyed the working Green implementation.

### Root Cause

Two independent failures:
1. Prompt code templates were authored without reference to the project's Biome lint config.
2. No lint gate existed for the Red phase. `bun run verify` cannot be used at Red because it includes tests that are expected to fail.

### Resolution

Added `bun run red-verify` as the full verify pipeline minus the test step. Red exit criteria now requires `bun run red-verify` to pass. Prompt code templates were updated to be lint-clean.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/phase-execution.md` should update Red exit criteria to require the full lint/format/typecheck pipeline, not just typecheck. If the project has a `red-verify` (or equivalent) script, use it. If not, run format + lint + typecheck individually.
- `references/story-prompts.md` should note that code templates in prompts must be checked against the target project's lint config before prompt finalization. A prompt that produces lint-failing code will cascade failures downstream.
- The self-review prompt template should run the project's lint pipeline, not just typecheck and tests.

---

## G-19: TDD Green prompts do not enforce Red-test immutability

**Date:** 2026-02-08
**Context:** Story 2b recovery and cross-story prompt review. Multiple Green prompts used permissive language ("prefer not to modify tests") and lacked a hard verification gate to detect changed test files during Green.

### What Happened

The process goal is to treat Red tests as the behavioral contract for Green and prevent implementation from drifting by changing tests. However, Green prompts did not enforce this consistently. Some prompts explicitly allowed limited test edits, and verification steps only ran `bun run verify`/`typecheck` without a dedicated "no test file changes" rail. This allows subtle reward-hacking and weakens traceability from Red -> Green.

### Root Cause

Two gaps in prompt/process guidance:
1. Prompt-writing guidance did not define "Red tests are immutable in Green" as a hard rule.
2. Green verification lacked a command-level guard that fails when test files are edited.

### Resolution

Added repository guard scripts:
- `guard:no-test-changes` — fails if test files are modified.
- `green-verify` — runs `verify` then `guard:no-test-changes`.

Updated Green prompts to require:
- "Do NOT modify test files in Green. Red tests are the contract."
- `bun run green-verify` as the Green quality gate.
- Done criteria explicitly confirming no test files were modified.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/story-prompts.md` should define an explicit Green prompt rule: Red tests are immutable during Green (no test edits).
- `references/phase-execution.md` should require a Green verification rail that checks for test-file diffs (project script if present, otherwise direct git-status/diff check).
- Green self-review/verification templates should include a mandatory "test files unchanged" check before a story can be marked Green complete.

---

## G-20: Agent Output Transparency — File-Based Reports + Forced Observation Pass

**Date:** 2026-02-08
**Context:** Story 3 and Story 4 execution. Orchestrator was reading full Codex execution transcripts (~300-400 lines of repeated diffs, intermediate tool calls, thinking blocks) to extract a small final summary. This wastes orchestrator context tokens and risks forcing auto-compaction before a story finishes. Separately, the self-review "READY" verdict had no mechanism to surface observations the agent considered too small to mention — the same filtering problem as G-17 but at the implementing/reviewing agent level rather than the orchestrator level.

### What Happened

1. **Transcript bloat:** Codex output files contain the full execution transcript — every tool call, every intermediate exec output, every diff applied, duplicated across status checks. The orchestrator `tail`s 80-150 lines to find the final summary, but this is noisy and context-expensive. When the orchestrator then independently reads all modified source files to "verify" the self-review, context burn doubles.

2. **Agent self-censoring:** When asked "what issues did you not report because you thought they were too small?", the self-review had genuinely found and reported everything. But the workflow had no structural guarantee of this — it relied on the agent's judgment about what counts as worth reporting, which is the same filtering bias that G-17 identified at the orchestrator level.

### Root Cause

Two gaps:
1. No mechanism for agents to produce a structured, file-based final report separate from their execution transcript.
2. No explicit second pass where the agent is forced to surface everything it observed but didn't include, with clear criteria for what qualifies.

### Proposed Solution: Two-Step Same-Session Pattern

**Step 1: File-based report.** Every execution and self-review prompt ends with:

```
As your final action, write your complete report to /tmp/lb-story-{N}-{phase}-report.md
Include: verdict (READY/NOT READY), issues found and fixed, issues found and NOT fixed,
quality gate results (with exact command output), test counts, and any observations.
```

The orchestrator reads one clean file instead of parsing the transcript. The transcript is only consulted if something goes wrong and debugging is needed.

**Step 2: Forced observation pass.** Immediately after the report, resume the same session with:

```
You just wrote your report. Now do a full transparency pass.

List EVERYTHING you noticed during this phase that you did not include in your report.
This includes observations you considered too small, nits, concerns you downgraded,
patterns that felt off but weren't wrong, anything.

Inclusion filter: Would fixing/addressing this take less than 15 minutes AND
produce any measurable improvement to correctness, consistency, maintainability,
or UX? If yes, include it.

Exclusion filter: Only exclude things that are purely cosmetic with zero
functional impact, OR would require significant redesign.

Write to /tmp/lb-story-{N}-{phase}-observations.md
```

The orchestrator reads both files and presents them to the human without filtering. The human triages.

### Why This Works

- **Kills transcript parsing entirely.** Orchestrator reads 2 small structured files instead of hundreds of lines of noisy transcript.
- **Preserves orchestrator context.** No need to independently read source files that the agent already reviewed — the report has the structured findings.
- **Forces transparency at the agent level.** The second pass with explicit inclusion/exclusion criteria makes it structurally impossible for the agent to silently filter. It's a direct prompt, not an instruction to "be thorough" (which is the same instruction that gets overridden by forward-momentum bias).
- **Separates "what happened" from "what else did you notice."** The report is the professional summary. The observations file is the unfiltered brain-dump. Both are valuable; they serve different purposes.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/story-prompts.md` should define file-based report output as a standard prompt suffix for all execution and review phases.
- `references/execution-orchestration.md` should document the two-step pattern (report + forced observation pass) as the standard agent output protocol.
- `references/phase-execution.md` should update self-review exit criteria to include both the report file and the observation file.
- Report and observation file paths should follow a predictable convention (e.g., `/tmp/lb-story-{N}-{phase}-report.md`) so the orchestrator can read them without parsing transcript output.

---

## Suggested Acceptance Criteria For These Skill Updates

1. A new feature using liminal-spec cannot finish Phase 3 without explicit `verify` and `verify-all` commands.
2. Story 0 prompts require runnable verification commands, even if integration/e2e are placeholders.
3. Tech Design includes an explicit "split/no-split" decision with thresholds and rationale.
4. Verification checkpoints fail if these process artifacts are absent.
5. Parallel agent plans define hard file/task boundaries and a single explicit post-parallel coherence step.
6. Story execution guidance requires a Red-phase commit boundary before Green implementation begins.
7. Green phase treats Red tests as immutable: test files are not edited during Green implementation.
8. Validator SOP documents required permissions for git/history checks, and validation reports must state when those checks could not be performed.
9. Red phase exit requires full lint/format/typecheck verification (not just typecheck). Projects should define a `red-verify` command or equivalent that runs everything except tests.
10. Green phase exit requires a hard test-immutability rail (for example, `green-verify`) that fails if any test files changed.
11. All execution and review phases produce a structured file-based report (not embedded in transcript output). The orchestrator reads the report file, not the transcript.
12. Every self-review includes a forced observation pass with explicit inclusion/exclusion criteria, producing a separate observations file. The orchestrator presents both files to the human without filtering.
