# Liminal Spec Gap Log (2026-02-07)

## Purpose

Track gaps found in this project's Feature Spec and Tech Design, then decide whether each gap should be addressed by improving the `liminal-spec` skill itself.

## Snapshot

This review compared:
- `docs/tech-design-mvp.md`
- `~/promptdb/docs/tech-arch/*.md`
- current local stack and package strategy

The key pattern: core protocol and flow design quality is strong, but cross-cutting architecture controls were under-specified until late (dependency baseline, NFR targets, threat model, env contract, and error contract).

## Gaps Found In Current Spec/Design

| ID | Gap | Impact | Added to Project Docs | Candidate Skill Improvement? |
|----|-----|--------|-----------------------|------------------------------|
| G-01 | No explicit dependency baseline strategy (runtime vs dev, version policy, optional packages) | Late churn in package choices and scripts | Yes | Yes |
| G-02 | Testing runner/tooling strategy decided late (Bun test vs Vitest projects) | Test organization and CI shape unclear | Partial | Yes |
| G-03 | Missing non-functional targets (latency, recovery, startup expectations) | Hard to evaluate readiness and regressions | Yes | Yes |
| G-04 | Missing constraints-accepted section | Hidden trade-offs stay implicit | Yes | Yes |
| G-05 | Missing threat model + input validation matrix | Security and robustness checks are ad hoc | Yes | Yes |
| G-06 | Missing runtime/env prerequisites matrix | Setup friction and hidden assumptions | Partial | Yes |
| G-07 | Missing machine-readable error contract (codes) | Inconsistent client error handling | Yes | Yes |
| G-08 | No explicit architecture pattern gate before execution | Important questions asked too late | Yes | Yes |
| G-09 | No explicit verification tiers (`verify` vs `verify-all`) in design-time planning | Inconsistent quality gates across stories and handoffs | Partial | Yes |
| G-10 | No policy for breaking up large tech design docs | Design readability and maintainability degrade as scope grows | Yes | Yes |
| G-11 | Parallel-agent guidance can over-emphasize coordination instead of hard task boundaries | Higher communication overhead, scope creep, and merge friction | Yes | Yes |
| G-12 | No explicit TDD phase commit boundary (Red completion checkpoint before Green implementation) | Harder audit trail and weaker protection against drifting directly to implementation | Yes | Yes |
| G-13 | No anti-reward-hacking check tied to modified test files during TDD Green | Risk of implementation gaming tests rather than satisfying intent | Yes | Yes |
| G-14 | Validator SOP does not explicitly include permissions needed for git-history checks | Validation quality depends on ad hoc permission assumptions | Yes | Yes |
| G-21 | Cross-story integration gap — shell.js postMessage relay unassigned to any story | End-to-end chat path broken: sessions create but can't be interacted with | Yes | Yes |

## What PromptDB Did Well That We Should Reuse

1. Trade-off tables with explicit "Constraints Accepted".
2. Requirements section including measurable NFR targets.
3. Threat model and input-validation coverage tied to tests.
4. Environment variable and runtime contract as first-class design artifact.
5. Error response catalog with stable codes/messages.
6. Tooling consistency (Biome single tool for lint+format, Vitest projects for test segmentation).

## Proposed `liminal-spec` Skill Refinements

### Phase 2 (Feature Specification) additions

1. Add a required "Architecture Context Capture" mini-section:
   - Existing stack and runtime constraints
   - Required compatibility targets
   - Hard dependency preferences (if any)
2. Promote NFRs from optional to default-required for system features.
3. Add a mandatory "Assumptions to Validate Before Story 0" table.

### Phase 3 (Tech Design) additions

1. Add a required "Dependency and Tooling Baseline" section:
   - Runtime dependencies
   - Dev dependencies
   - Version policy (pinning strategy)
   - Script contract
   - Verification tiers (`verify`, `verify-all`) with exact command composition
2. Add required cross-cutting sections:
   - Constraints Accepted
   - Threat Model + Input Validation Matrix
   - Runtime Prerequisites + Environment Contract
   - Error Contract (machine-readable codes)
   - Non-functional target table
3. Add a "Phase 3 Gate: Architecture Pattern Checklist" before handoff to story sharding.
4. Add required "Design Document Decomposition Plan":
   - break-out thresholds
   - split strategy (by domain/altitude)
   - source-of-truth index doc

## Verification Protocol (Required in Tech Design)

Every Tech Design must define two verification tiers before Story 0 execution:

1. `verify` (regular verification): fast feedback gate used continuously during implementation.
2. `verify-all` (deep verification): broader gate for integration depth and release confidence.

### Required shape

- `verify` must include at least: `format:check`, `lint`, `typecheck`, and primary mocked/service test suites.
- `verify-all` must run `verify` plus integration suites, and include e2e suites when available.
- If integration/e2e suites do not exist yet, `verify-all` must still exist and run placeholders/no-op commands that return success with clear output.

### Example from this project

In `/Users/leemoore/code/liminal-builder/package.json`:

- `verify`: `bun run format:check && bun run lint && bun run typecheck && bun run test`
- `verify-all`: `bun run verify && bun run test:integration && bun run test:e2e`

This should be identified and created during Tech Design, not discovered ad hoc during execution.

## Tech Design Decomposition Policy (Required)

Tech designs naturally become large; decomposition must be intentional.

### Decision triggers for breakout docs

Break out into additional documents when any of these occurs:

1. Primary Tech Design exceeds ~1,500-2,000 lines or navigation overhead becomes material.
2. One domain (auth, protocol, UI architecture, testing, infra) grows dense enough to distract from core flow.
3. A section needs independent review cadence or ownership.

### Breakout method

1. Keep one index/root Tech Design document as the canonical map and decision log.
2. Split into focused companion docs (for example: `tech-design-testing.md`, `tech-design-protocol.md`, `tech-design-ui.md`).
3. Add explicit cross-links and "source of truth" ownership notes per section.
4. Preserve requirement traceability in each breakout doc (AC/TC mappings where relevant).

## Parallel Agent Planning Principle

When recommending parallel agent workflows, prefer hard-bounded work partitions over communication-heavy coordination patterns.

### Guidance

1. Partition work into independent paths with explicit file ownership.
2. Enforce strict no-cross-scope editing during parallel execution.
3. Avoid asking parallel agents to resolve cross-cutting coherence in-flight.
4. Run one explicit post-parallel coherence pass after all bounded tasks complete.

### Why

This reduces quadratic coordination costs, lowers merge/conflict risk, and keeps parallelism focused on truly independent work.

## TDD Integrity And Validator Permission Controls

### TDD phase boundary requirement

1. At the end of TDD Red for a story, create a commit checkpoint before entering TDD Green.
2. The checkpoint commit should include failing/expected-red tests and any skeleton scaffolding needed to express the behavior contract.
3. TDD Green proceeds only after this boundary exists, preserving a clear audit trail from requirement to red tests to implementation.

### Anti-reward-hacking checks in TDD Green

1. If test files are modified during Green, explicitly validate those test changes against the story AC/TC intent.
2. Validation must include review of changed assertions and coverage shape to ensure tests did not weaken intent.
3. Validators should inspect recent git history for affected test files to detect abrupt intent regression patterns.

### Validator permissions in SOP

1. Validator/subagent SOP guidance must declare required permissions for git/history inspection commands.
2. Typical minimum capability includes running `git log`, `git show`, and `git blame` on relevant test paths.
3. If permissions are restricted, SOP must define fallback behavior (flag incomplete validation instead of silently skipping history checks).

### Phase 4/5 (Story Sharding/Execution) additions

1. Require Story 0 prompt to include explicit setup verification:
   - dependency install success
   - scripts present and runnable
   - format/lint/typecheck/test baseline commands
2. Require validator prompts to check cross-cutting sections, not only module/interface completeness.

## New "Must Ask" Questions For Future Runs

Use these before freezing Phase 2 or Phase 3:

1. What existing stack patterns in adjacent projects should be reused?
2. Which runtime and package ecosystem versions are the compatibility target?
3. Do we need schema-first validation at all external boundaries? If yes, with what library?
4. How should tests be segmented (unit/ui/integration/e2e) and with what runner architecture?
5. What are the measurable NFR targets for startup, latency, reliability, and recovery?
6. What threats are in scope, and where are controls enforced and tested?
7. What env/runtime prerequisites are required locally and in CI?
8. What stable error code contract should clients rely on?
9. What is the `verify` command for regular development verification?
10. What is the `verify-all` command for deep verification, and what does it add beyond `verify`?
11. At what thresholds will Tech Design be split into breakout documents, and what is the split plan?
12. If parallel agents are used, what are the exact hard scope boundaries for each agent?
13. What is the single post-parallel coherence/reconciliation step?
14. Where is the TDD Red->Green commit boundary for each story, and what commit evidence is required?
15. If tests were changed in TDD Green, how will validators confirm those changes preserved AC/TC intent instead of weakening checks?
16. What validator agent permissions are required to run git-history checks on modified test files?
17. For each sequence diagram in the tech design, does every participant module have pseudocode for its role in the flow, or are some described only narratively?
18. After story sharding, can you trace every arrow in the primary sequence diagram to a specific story's Modified Files list?

## Tracking Loop

For each newly discovered gap during this build:
1. Record it here with impact and mitigation.
2. Ask: "Can this be solved by improving `liminal-spec` guidance/checklists/templates?"
3. If yes, draft a specific skill update proposal (section + wording + phase).
4. Re-evaluate at story boundaries (after Story 0, after first integration story, before full execution wave).

## Next Candidate Improvements To Draft

1. Update `references/tech-design.md` with a cross-cutting architecture checklist template.
2. Update `references/feature-specification.md` to require architecture-context capture + default NFR table.
3. Add validation prompt snippets for dependency/tooling baseline and threat/input-validation review.
4. Add a standard verification-tier template (`verify` + `verify-all`) to Tech Design guidance and Story 0 prompt guidance.
5. Add a "when/how to split Tech Design docs" section to prevent oversized monolith design docs.
6. Add explicit parallelization guidance: hard boundaries first, one coherence pass after parallel execution.
7. Add TDD integrity guidance: mandatory Red->Green commit boundary plus validator review of changed tests in Green.
8. Add validator SOP guidance that explicitly includes permissions for git-history inspection of relevant test files.
9. Add Integration Path Trace as a required story sharding gate — table format mapping sequence diagram arrows to story owners, modified files, and tests.
10. Add tech design guidance requiring uniform pseudocode depth for all sequence diagram participants, especially relay/bridge modules.

## Recommended Skill File Changes (for Review)

| File | Recommended Change | Why |
|------|--------------------|-----|
| `/Users/leemoore/.claude/skills/liminal-spec/references/tech-design.md` | Add required "Cross-Cutting Architecture Controls" section with: dependency/tooling baseline, script contract, verification tiers (`verify`/`verify-all`), and doc decomposition policy. Extend handoff checklist to confirm these are defined. | G-01, G-02, G-09, and G-10 happened because architecture/process gates were implied but not explicitly required in Phase 3 guidance. |
| `/Users/leemoore/.claude/skills/liminal-spec/templates/tech-design.template.md` | Add template placeholders/tables for: resolved Tech Design Questions (including verification tiers + decomposition), script contract table, and decomposition thresholds. Add self-review checklist items for these sections. | Template-level nudges are the fastest way to make these controls consistently appear without relying on memory in each run. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/feature-specification.md` | In "Tech Design Questions" guidance/template, include required process questions: `verify`, `verify-all`, and split-plan thresholds. | If these questions are captured in Phase 2, Phase 3 must answer them before execution pressure starts. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/story-sharding.md` | Update Story 0 guidance so setup includes verification scripts and verify prompt expects regular/deep verification gates to be wired, with placeholders allowed when suites do not yet exist. | Story 0 is where project execution contract is created; missing script contract there causes drift and late rework. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/verification.md` | Add pre-story checkpoint items that confirm verification-tier commands are defined and a decomposition decision is documented. | This makes the phase gate enforce the process quality bar, not just module/interface completeness. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Add explicit orchestration rules for parallel execution: hard scope boundaries, no cross-scope edits, and one post-parallel coherence pass. | Parallel execution quality depends more on boundary discipline than in-flight coordination between many agents. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/phase-execution.md` | Add explicit Red->Green boundary control: commit at Red completion, then require Green validators to review any changed tests against AC/TC intent plus git-history signals. | Reinforces TDD integrity and reduces reward-hacking risk by making test-change scrutiny part of the process contract. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/execution-orchestration.md` | Extend validator SOP section to include required permissions/capabilities for test-file history inspection (`git log/show/blame`) and explicit fallback behavior when unavailable. | High-confidence validation requires historical context on test changes; this must be explicit, not assumed. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/story-sharding.md` | Add "Integration Path Trace" as a required step between story definition and prompt writing. For each critical user path, every segment must map to a story's Modified Files list and have at least one test. Empty cells are blockers. | G-21: cross-story integration gaps are invisible to per-story completeness checks. The trace catches seams between stories that no individual story owns. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/verification.md` | Add to "Before Execution" checkpoint: "Integration path trace complete — every segment of critical user paths has a story owner." | G-21: the existing checkpoint verifies story-level completeness but not cross-story integration coverage. |
| `/Users/leemoore/.claude/skills/liminal-spec/references/tech-design.md` | Add guidance: all modules that appear as participants in sequence diagrams should have pseudocode at the same detail level. Relay/bridge modules especially must not be described only narratively while their endpoints get full pseudocode. | G-21: the asymmetry between detailed portlet.js/tabs.js pseudocode and narrative-only shell.js description enabled the sharding agent to overlook the relay as a concrete deliverable. |

## G-15: Story Validation Process (Agent Teams + Dual-Validator)

**Date:** 2026-02-07
**Context:** Ran full dual-validator story verification across 7 stories using Claude Code Agent Teams in tmux split panes + Codex GPT-5.3 subagents.

### What Happened
- 4 Opus teammates in tmux split panes, each launching 1-2 Codex (gpt-5.3-codex, high reasoning) subagents
- Phase 1: 7 parallel Codex verification runs (read-only, `-o` for output)
- Phase 2: Each Opus teammate validated every Codex claim against actual source files
- Phase 3: Adversarial debate — Codex sessions resumed with Opus pushback, Codex defended or conceded with evidence
- Phase 4: Fix list consolidated — all valid issues fixed, no severity-based deferrals

### Key Findings
1. Codex overstated severity on ~40% of issues; Opus caught design intent Codex missed
2. Codex was RIGHT and Opus was WRONG on 1 issue (TS compiler check on Record<CliType>)
3. Neither model alone was sufficient — the dual-validator pattern is essential
4. "Non-blocking" is not a valid reason to skip fixes. Fix everything unless genuinely unimportant AND difficult.
5. Wall-clock: ~30 min for what would be 4+ hours sequential

### Candidate Skill Improvement
- Draft reference doc: `.research/outputs/draft-validation-process-reference.md`
- Covers: agent teams setup, parallel Codex dispatch, dual-validator pattern, adversarial debate protocol, fix-everything standard
- To be reviewed for possible inclusion in liminal-spec skill as `references/story-validation-teams.md`

## G-16: Per-Story Implementation Workflow (Self-Review + Dual Verification)

**Date:** 2026-02-07
**Context:** Executed Story 1 (Project Sidebar) using a structured per-story workflow with Codex implementation, same-session self-review, and parallel dual verification.

### What Was Documented

The existing skill references (`phase-execution.md`, `execution-orchestration.md`) define the story cycle phases and agent coordination patterns, but do not specify:

1. **Same-session self-review** — After each implementation phase (Red, Green), the implementing agent critically reviews its own work before handoff. Cheaper than verification, catches 60-70% of issues.
2. **Dual verification details** — Codex + Senior Engineer (Opus) run the same verify prompt in parallel. Consolidated report presented to human.
3. **Human-gated fix cycles** — Human decides fix agent, scope, and when to move on. 1-3 rounds typical.
4. **Session management specifics** — When to resume vs fresh session, `resume` flag limitations, session ID tracking.
5. **Commit boundary protocol** — Required commits at Red and Green checkpoints with specific message conventions.
6. **Test modification policy for Green** — Green should not modify tests; environmental fixes flagged, assertion changes rejected.

### Gap

The skill's Phase 5 references describe *what* happens (Skeleton → Red → Green → Verify) and *how agents coordinate* (dual-validator, pipeline), but not the *specific operational workflow* including self-review passes, session resume patterns, commit discipline, and the human-gated fix cycle. This is the missing "how to actually run a story" reference.

### Draft Document

`docs/liminal-spec-updates/draft-story-execution-workflow.md`

### Candidate Skill Improvement?

**Yes.** This should become a companion reference to `phase-execution.md` and `execution-orchestration.md`, or be merged into one of them. The workflow is model-agnostic (works with any Codex-class + Opus-class pairing) and applies to any project using the liminal-spec methodology.

### Observations from Story 1

1. Self-review caught and fixed 5 issues + 2 nits during Red, and 3 issues during Green — all before verification
2. Dual verification found 1 legitimate production bug (Codex C) and 3 nits (Senior Engineer)
3. Codex resume syntax has gotchas: no `-C`, `--sandbox`, `-m`, `-c`, or `-o` flags on resume
4. Green modified test files (environment shims) — detected but current prompts are too permissive about this (see G-13)

---

## G-17: Orchestrator Transparency — Small Issues Buried During Execution

**Date:** 2026-02-07
**Context:** Story 2a (ACP Client) execution through full workflow. Post-execution review revealed the orchestrator (Opus) had noticed but not raised multiple issues during the run.

### What Happened

After completing Story 2a through dual verification, a direct question — "give me a list of all the issues you aren't telling me about" — surfaced 10 items the orchestrator had observed but not reported. These ranged from process gaps (Red never ran `bun run verify`) to spec violations (`close()` ignoring its `timeoutMs` parameter) to code quality concerns (`any` types in the prompt template, dead code from Story 1). Some were real problems requiring fixes; others were already clean but the orchestrator hadn't verified.

Additionally, 13 unresolved issues from Story 1 execution were discovered in a separate review — including a data path deviation from spec, dead test shims, exposed internal state (`public store`), and missing test cleanup — none of which the orchestrator had inventoried or raised proactively.

### Root Cause

The orchestrator defaults to forward momentum. When implementation agents or verifiers categorize something as a "nit" or "acceptable for MVP," the orchestrator accepts that framing without pushing back or accumulating the item for human review. The workflow doc defines the human as the gate, but the orchestrator filters what reaches the human.

### What Should Change

1. **Post-phase issue inventory.** After each major phase (Red complete, Green complete, verification complete), the orchestrator must produce a complete inventory of everything observed — not just blockers. Include items verifiers downgraded, process deviations, spec divergences, and code quality notes. Let the human decide what's small.

2. **No agent-determined severity downgrades.** When a verifier calls something a "nit," the orchestrator should report it at face value to the human rather than silently accepting the downgrade. The human may disagree about severity.

3. **Cross-story accumulation.** Issues that affect shared infrastructure (Story 0 code, store behavior, data paths, test patterns) must be flagged as cross-story concerns, not buried in per-story notes. These compound if deferred.

4. **Explicit "what I noticed but didn't flag" checkpoint.** Before the orchestrator declares a story complete, it should self-audit: "What did I observe during this execution that I haven't surfaced?" This is cheap insurance against the forward-momentum bias.

### Likely Fix: User-Initiated Prompt

Instructing the orchestrator to self-audit is unreliable — the same forward-momentum bias that causes the filtering will undermine instructions to stop filtering. This is probably best addressed with a **standard configured prompt that the user runs at the end of each story** (or at phase boundaries), something like:

> "List every issue you observed during this execution that you haven't raised — process gaps, spec divergences, severity downgrades by verifiers, code quality concerns, cross-story impacts, things you considered small. Do not filter. I will triage."

This should be documented as a required user action in the workflow, not as an orchestrator self-check. The user is the forcing function.

### Candidate Skill Improvement?

**Yes.** The execution orchestration reference (`execution-orchestration.md`) and the story execution workflow doc should include:
- A documented "transparency prompt" the user runs at story completion (not optional)
- A prohibition on the orchestrator filtering severity — report everything, let the human triage
- A cross-story concern accumulator that persists across story boundaries

### Items Found During This Review

| Item | Category | Resolution |
|------|----------|------------|
| Red phase never ran `bun run verify` | Process gap | Fixed — added `bun run verify` to all Red prompts (9 files) |
| `close(timeoutMs)` ignored timeout | Spec violation | Fixed — implemented `Promise.race` timeout |
| `handleAgentRequest` missing params | Spec divergence | Fixed — params now passed through |
| Data path `./data/` vs `~/.liminal-builder/` | Spec deviation | Fixed — corrected to homedir path |
| Dead Bun shim in project-store tests | Dead code | Fixed — removed |
| `public store` on ProjectStore | Encapsulation leak | Fixed — changed to private, tests use public API |
| JsonStore `writeDebounceMs <= 0` undocumented | Cross-story concern | Fixed — documented in JSDoc |
| Missing `afterEach` in client tests | Test hygiene | Fixed — added |
| Vitest/Node vs Bun runtime mismatch | Systemic | Documented in tech design |
| `any` types in green prompt template | Prompt quality | Not a current code issue (Codex B used proper types), but prompt should be cleaned for reuse |

---

## G-18: No lint verification gate at TDD Red phase

**Date:** 2026-02-07
**Context:** Story 2b (Agent Manager) execution. Skeleton+Red prompt contained `any` types throughout DI interfaces and test mocks. Red completed with passing typecheck and correct test failure pattern, but Biome lint (with `--error-on-warnings`) rejected the code. This was not discovered until after Green completed, causing churn and a lost Green implementation.

### What Happened

The Red exit criteria only checked typecheck and tests. The project's `bun run verify` includes Biome lint with `--error-on-warnings`, which treats `noExplicitAny` as a hard failure. The Red prompt's code templates used `any` freely at DI boundaries and in test mocks. The Red self-review ran typecheck and tests but not lint. The problem cascaded forward: Green implemented against lint-dirty code, verification failed on lint, then the orchestrator tried to fix lint issues directly and destroyed the working Green implementation.

### Root Cause

Two independent failures:
1. Prompt code templates were authored without reference to the project's Biome lint config.
2. No lint gate existed for the Red phase. `bun run verify` cannot be used at Red because it includes tests that are expected to fail.

### Resolution

Added `bun run red-verify` as the full verify pipeline minus the test step. Red exit criteria now requires `bun run red-verify` to pass. Prompt code templates were updated to be lint-clean.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/phase-execution.md` should update Red exit criteria to require the full lint/format/typecheck pipeline, not just typecheck. If the project has a `red-verify` (or equivalent) script, use it. If not, run format + lint + typecheck individually.
- `references/story-prompts.md` should note that code templates in prompts must be checked against the target project's lint config before prompt finalization. A prompt that produces lint-failing code will cascade failures downstream.
- The self-review prompt template should run the project's lint pipeline, not just typecheck and tests.

---

## G-19: TDD Green prompts do not enforce Red-test immutability

**Date:** 2026-02-08
**Context:** Story 2b recovery and cross-story prompt review. Multiple Green prompts used permissive language ("prefer not to modify tests") and lacked a hard verification gate to detect changed test files during Green.

### What Happened

The process goal is to treat Red tests as the behavioral contract for Green and prevent implementation from drifting by changing tests. However, Green prompts did not enforce this consistently. Some prompts explicitly allowed limited test edits, and verification steps only ran `bun run verify`/`typecheck` without a dedicated "no test file changes" rail. This allows subtle reward-hacking and weakens traceability from Red -> Green.

### Root Cause

Two gaps in prompt/process guidance:
1. Prompt-writing guidance did not define "Red tests are immutable in Green" as a hard rule.
2. Green verification lacked a command-level guard that fails when test files are edited.

### Resolution

Added repository guard scripts:
- `guard:no-test-changes` — fails if test files are modified.
- `green-verify` — runs `verify` then `guard:no-test-changes`.

Updated Green prompts to require:
- "Do NOT modify test files in Green. Red tests are the contract."
- `bun run green-verify` as the Green quality gate.
- Done criteria explicitly confirming no test files were modified.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/story-prompts.md` should define an explicit Green prompt rule: Red tests are immutable during Green (no test edits).
- `references/phase-execution.md` should require a Green verification rail that checks for test-file diffs (project script if present, otherwise direct git-status/diff check).
- Green self-review/verification templates should include a mandatory "test files unchanged" check before a story can be marked Green complete.

---

## G-20: Agent Output Transparency — File-Based Reports + Forced Observation Pass

**Date:** 2026-02-08
**Context:** Story 3 and Story 4 execution. Orchestrator was reading full Codex execution transcripts (~300-400 lines of repeated diffs, intermediate tool calls, thinking blocks) to extract a small final summary. This wastes orchestrator context tokens and risks forcing auto-compaction before a story finishes. Separately, the self-review "READY" verdict had no mechanism to surface observations the agent considered too small to mention — the same filtering problem as G-17 but at the implementing/reviewing agent level rather than the orchestrator level.

### What Happened

1. **Transcript bloat:** Codex output files contain the full execution transcript — every tool call, every intermediate exec output, every diff applied, duplicated across status checks. The orchestrator `tail`s 80-150 lines to find the final summary, but this is noisy and context-expensive. When the orchestrator then independently reads all modified source files to "verify" the self-review, context burn doubles.

2. **Agent self-censoring:** When asked "what issues did you not report because you thought they were too small?", the self-review had genuinely found and reported everything. But the workflow had no structural guarantee of this — it relied on the agent's judgment about what counts as worth reporting, which is the same filtering bias that G-17 identified at the orchestrator level.

### Root Cause

Two gaps:
1. No mechanism for agents to produce a structured, file-based final report separate from their execution transcript.
2. No explicit second pass where the agent is forced to surface everything it observed but didn't include, with clear criteria for what qualifies.

### Proposed Solution: Two-Step Same-Session Pattern

**Step 1: File-based report.** Every execution and self-review prompt ends with:

```
As your final action, write your complete report to /tmp/lb-story-{N}-{phase}-report.md
Include: verdict (READY/NOT READY), issues found and fixed, issues found and NOT fixed,
quality gate results (with exact command output), test counts, and any observations.
```

The orchestrator reads one clean file instead of parsing the transcript. The transcript is only consulted if something goes wrong and debugging is needed.

**Step 2: Forced observation pass.** Immediately after the report, resume the same session with:

```
You just wrote your report. Now do a full transparency pass.

List EVERYTHING you noticed during this phase that you did not include in your report.
This includes observations you considered too small, nits, concerns you downgraded,
patterns that felt off but weren't wrong, anything.

Inclusion filter: Would fixing/addressing this take less than 15 minutes AND
produce any measurable improvement to correctness, consistency, maintainability,
or UX? If yes, include it.

Exclusion filter: Only exclude things that are purely cosmetic with zero
functional impact, OR would require significant redesign.

Write to /tmp/lb-story-{N}-{phase}-observations.md
```

The orchestrator reads both files and presents them to the human without filtering. The human triages.

### Why This Works

- **Kills transcript parsing entirely.** Orchestrator reads 2 small structured files instead of hundreds of lines of noisy transcript.
- **Preserves orchestrator context.** No need to independently read source files that the agent already reviewed — the report has the structured findings.
- **Forces transparency at the agent level.** The second pass with explicit inclusion/exclusion criteria makes it structurally impossible for the agent to silently filter. It's a direct prompt, not an instruction to "be thorough" (which is the same instruction that gets overridden by forward-momentum bias).
- **Separates "what happened" from "what else did you notice."** The report is the professional summary. The observations file is the unfiltered brain-dump. Both are valuable; they serve different purposes.

### Candidate Skill Improvement?

**Yes.** For the next liminal-spec skill development cycle:
- `references/story-prompts.md` should define file-based report output as a standard prompt suffix for all execution and review phases.
- `references/execution-orchestration.md` should document the two-step pattern (report + forced observation pass) as the standard agent output protocol.
- `references/phase-execution.md` should update self-review exit criteria to include both the report file and the observation file.
- Report and observation file paths should follow a predictable convention (e.g., `/tmp/lb-story-{N}-{phase}-report.md`) so the orchestrator can read them without parsing transcript output.

---

## G-21: Cross-Story Integration Gap — Shell.js PostMessage Relay Unassigned

**Date:** 2026-02-08
**Context:** Post-Story 4 gorilla testing. User created sessions via the sidebar but could not interact with them because the shell.js postMessage relay between the WebSocket and portlet iframes was never assigned to any story.

### What Happened

After completing Stories 0-4, gorilla testing revealed that creating a session works (sidebar → server → ACP → session appears in sidebar), but clicking the session produces no chat UI. The full end-to-end path requires a bidirectional relay in `shell.js` that:

1. Receives WebSocket messages (e.g., `session:update`, `session:chunk`, `session:complete`) and routes them to the correct portlet iframe via `postMessage`
2. Receives `postMessage` events from portlet iframes (e.g., `session:send`, `session:cancel`), injects the `sessionId` (which the portlet doesn't know), and forwards to the WebSocket
3. Resolves which iframe sent a `postMessage` by checking `event.source` against the iframe Map (since `PortletToShell` messages contain no sessionId)

This relay is the central integration point — without it, the server-side plumbing (Stories 2a, 2b, 4) and the client-side rendering (Story 3) are isolated pieces that never connect.

**No story assigns this work.** Tracing through every story:

| Story | shell.js scope | Relay covered? |
|-------|---------------|----------------|
| Story 0 | Stub: WebSocket connection setup | No |
| Story 3 | Not modified (portlet side only) | No |
| Story 4 | Not modified (sidebar + server) | No |
| Story 5 | Listed as "Existing file (unchanged)" with constraint "Do NOT modify files outside tabs.js and shell.css" | **Explicitly excluded** |
| Story 6 | shell.js: "WebSocket reconnection logic" only | No |

Additionally, Story 5 does not auto-open a tab when `session:created` arrives — the feature spec (TC-2.2a) says "A new session opens in the main area, a tab appears," but the sidebar's `session:created` handler only updates the session list. The `openTab()` call that should follow is not in any story.

### Root Cause

**The gap was introduced during story sharding (Phase 4), not in the feature spec or tech design.**

**Feature spec:** Clean. AC-4.1 covers tab creation on session open. The flows describe the full message path. The data contracts define the postMessage protocol.

**Tech design:** Correctly describes the relay at multiple levels:
- High-altitude data flow (lines 111-113): "shell routes via postMessage → portlet renders"
- Step-by-step message flow (lines 497-514): "shell.js receives, wraps as WebSocket message" and "shell.js receives, routes to correct portlet iframe via postMessage"
- Module responsibility matrix (line 420): shell.js does "message routing to portlets via postMessage"
- Two sequence diagrams (lines 807-842, 898-915) show shell.js as the relay participant
- Full postMessage contract with TypeScript types (lines 470-516)

**However, the tech design has an asymmetry that enabled the sharding gap.** It provides detailed pseudocode for `portlet.js` message handling (lines 762-800) and `tabs.js` iframe lifecycle (lines 948-998), but for `shell.js` it only states the *responsibility* ("message routing to portlets via postMessage") without pseudocode for:
- The `window.addEventListener('message', ...)` handler for portlet → shell messages
- The WebSocket `onmessage` dispatch to the correct iframe
- The reverse-lookup from `event.source` to sessionId
- The API contract between shell.js and tabs.js for iframe access (the iframe Map is owned by tabs.js)

This asymmetry — narrative description vs. concrete pseudocode — made it easy for the sharding agent to treat shell.js relay as something that would "just happen" rather than as a specific deliverable requiring explicit assignment.

**The sharding process then compounded the asymmetry:**
1. Each story took one side of the bridge: Story 3 took the portlet side, Story 5 took the tab lifecycle side
2. Story 5 explicitly listed shell.js as "unchanged" and constrained the implementing agent from touching it
3. No story validation step checked whether the seams between stories were covered
4. The per-story "demo-able on its own" principle (from feature-specification.md) was not enforced during sharding

### The Principle

**Narrative integration is not the same as assigned integration.** A tech design can perfectly describe how components interact (sequence diagrams, data flows, step-by-step message traces) while no story actually owns implementing the glue. The design describes the *desired* state. The stories describe the *assigned* work. If verification only checks that each story is internally complete and that the design is narratively coherent, the seams between stories fall through.

This is a category of gap distinct from within-story issues (which the AC → TC → Test chain catches). It's a **horizontal integration gap** — the cross-story equivalent of a missing foreign key. Each story satisfies its own ACs, but the user path that spans multiple stories has unassigned segments.

### Immediate Resolution

Before executing Story 5:
1. Expand Story 5 scope to include shell.js modifications: postMessage relay (WS ↔ portlet iframes), `initTabs()` dependency injection, and `session:created` triggering `openTab()`
2. Add 2-3 tests for the message relay (not just tab UI lifecycle)
3. Expand Story 6 to include `session:reconnect` server handler (websocket.ts) and SIGINT shutdown wiring (index.ts)

### Candidate Skill Improvement?

**Yes.** This is a Phase 4 (Story Sharding) process gap. The `story-sharding.md` reference needs an **Integration Path Trace** step.

### Proposed Addition to `references/story-sharding.md`

After stories are defined and before prompts are written, add a required verification step:

```markdown
### Integration Path Trace (Required)

After defining all stories, trace each critical end-to-end user path
through the story breakdown. This catches cross-story integration gaps
that per-story AC/TC coverage cannot detect.

1. List the 1-3 most important user paths (from the feature spec flows)
2. Break each path into segments (each arrow in the sequence diagram)
3. For each segment, identify which story owns it
4. Verify the owning story lists the relevant file in "Modified Files"
5. Verify at least one test in that story exercises the segment

Any segment with no story owner is an integration gap. Fix before
proceeding to prompts.

**Format:**

| Path Segment | Description | Owning Story | Modified File | Test |
|-------------|-------------|-------------|---------------|------|
| User → sidebar | Click "New Session" | Story 4 | sidebar.js | TC-2.2a |
| sidebar → server | session:create WS message | Story 4 | websocket.ts | TC-2.2a |
| server → ACP | AcpClient.sessionNew() | Story 2a | acp-client.ts | TC-2.2a |
| server → sidebar | session:created response | Story 4 | sidebar.js | TC-2.2a |
| sidebar → tab | Auto-open tab on create | ??? | ??? | ??? |
| WS → portlet | Shell relays to iframe | ??? | ??? | ??? |
| portlet → WS | Shell receives postMessage | ??? | ??? | ??? |

Empty cells ("???") are integration gaps.
```

### Why The Existing Process Didn't Catch This

The existing story sharding validation asks:
- "Can you map design chunks to discrete stories?" — Yes (each chunk mapped cleanly)
- "Are interfaces clear enough to write implementation prompts?" — Yes (within each story)
- "Is test mapping complete?" — Yes (every TC mapped to a test in some story)

These are all **within-story completeness** checks. The missing check is **cross-story integration coverage**: does the union of all stories produce a connected system, or just a collection of independently-correct components?

The "Before Execution" checkpoint (verification.md) checks:
- Stories and prompts complete — Yes
- Prompts are self-contained — Yes
- Different model reviewed prompts — Yes

None of these verify that the critical user path has every segment assigned.

### Additional Recommended Skill File Changes

| File | Recommended Change | Why |
|------|--------------------|-----|
| `references/story-sharding.md` | Add "Integration Path Trace" as a required step between story definition and prompt writing. Include the table format and the rule that empty cells are blockers. | This gap happened because no step in story sharding checks cross-story integration coverage. The trace is cheap (one table per feature) and would have caught this exact gap. |
| `references/verification.md` | Add to "Before Execution" checkpoint: "Integration path trace complete — every segment of critical user paths has a story owner." | The checkpoint should enforce the new gate. |
| `references/tech-design.md` | Add guidance that all modules in sequence diagrams should have pseudocode at the same level of detail, not just the "interesting" ones. Specifically: if a module appears as a relay/bridge participant in a sequence diagram, its relay logic should get the same pseudocode treatment as the modules it connects. | The asymmetry between detailed portlet.js/tabs.js pseudocode and narrative-only shell.js description enabled the sharding agent to overlook the relay as a concrete deliverable. |

### Must-Ask Questions (additions)

17. For each sequence diagram in the tech design, does every participant module have pseudocode for its role in the flow, or are some described only narratively?
18. After story sharding, can you trace every arrow in the primary sequence diagram to a specific story's Modified Files list?

---

## Suggested Acceptance Criteria For These Skill Updates

1. A new feature using liminal-spec cannot finish Phase 3 without explicit `verify` and `verify-all` commands.
2. Story 0 prompts require runnable verification commands, even if integration/e2e are placeholders.
3. Tech Design includes an explicit "split/no-split" decision with thresholds and rationale.
4. Verification checkpoints fail if these process artifacts are absent.
5. Parallel agent plans define hard file/task boundaries and a single explicit post-parallel coherence step.
6. Story execution guidance requires a Red-phase commit boundary before Green implementation begins.
7. Green phase treats Red tests as immutable: test files are not edited during Green implementation.
8. Validator SOP documents required permissions for git/history checks, and validation reports must state when those checks could not be performed.
9. Red phase exit requires full lint/format/typecheck verification (not just typecheck). Projects should define a `red-verify` command or equivalent that runs everything except tests.
10. Green phase exit requires a hard test-immutability rail (for example, `green-verify`) that fails if any test files changed.
11. All execution and review phases produce a structured file-based report (not embedded in transcript output). The orchestrator reads the report file, not the transcript.
12. Every self-review includes a forced observation pass with explicit inclusion/exclusion criteria, producing a separate observations file. The orchestrator presents both files to the human without filtering.
13. Story sharding includes an Integration Path Trace for each critical user path. Every segment (every arrow in the sequence diagram) must have a story owner, a modified file, and a test. Empty cells block prompt writing.
14. Tech design pseudocode coverage is uniform across sequence diagram participants — relay/bridge modules get the same pseudocode depth as the modules they connect.
15. Skeleton phase lint suppression hacks (`void` statements for unused helpers, `biome-ignore useConst` for `let` declarations) should not be necessary. One option to explore: a `biome-red.json` config that relaxes `noUnusedVariables` and `useConst` for `red-verify`, so skeletons are lint-clean without hacks and the Green diff is cleaner (no suppression removal noise).
16. Self-review prompts should trust the implementing model's project knowledge rather than enumerating specific checks. The user-approved self-review prompt for TDD Green phase is:

```
You just completed the TDD Green phase. Now do a thorough critical review of your own implementation.

If you find issues and the fix is not controversial or requiring a judgment call, fix them. Then report back: what issues you encountered, what you fixed, and any issues you encountered but didn't fix and why.

Do a thorough assessment for readiness to move to the next phase. Final verdict: READY or NOT READY for Verification.
```

The TDD Red phase self-review should follow the same pattern — trust the model to know what to check, don't over-prescribe. GPT-5.3-codex at high reasoning already knows the project standards; the prompt just needs to tell it to go deep and be critical, not enumerate what "deep" means.
17. Story 0 (or the architecture/stack standup phase) should include creating `CLAUDE.md` and `AGENTS.md` with project standards, conventions, and context. These files are how Claude Code and Codex respectively pick up project knowledge without it being repeated in every prompt. Without them, every agent starts cold and the orchestrator or prompt author has to compensate by being more prescriptive — which is the opposite of what works well with high-reasoning models that perform better with trust and latitude.
